{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fEoNgEU3uzpn"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (pywrap_tensorflow_internal.py, line 114)",
          "output_type": "error",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "\u001b[0m  File \u001b[1;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:3378\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
            "\u001b[0m  Cell \u001b[1;32mIn [4], line 8\u001b[0m\n    from tensorflow.keras.models import Sequential\u001b[0m\n",
            "\u001b[0m  File \u001b[1;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/__init__.py:24\u001b[0m\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\u001b[0m\n",
            "\u001b[0m  File \u001b[1;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/__init__.py:49\u001b[0m\n    from tensorflow.python import pywrap_tensorflow\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/pywrap_tensorflow.py:58\u001b[0;36m\n\u001b[0;31m    from tensorflow.python.pywrap_tensorflow_internal import *\u001b[0;36m\n",
            "\u001b[0;36m  File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:114\u001b[0;36m\u001b[0m\n\u001b[0;31m    def TFE_ContextOptionsSetAsync(arg1, async):\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Importing the required libraries\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_regression\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zvif-Ru_u1t_",
        "outputId": "61c103ce-e38d-4738-ed60-da7d89b21671"
      },
      "outputs": [],
      "source": [
        "# Defining the list of tickers\n",
        "tickers = ['AAPL', 'GOOG', 'AMZN', 'TSLA', 'MSFT',\"^GSPC\", \"^IXIC\"]\n",
        "\n",
        "# Collecting the data\n",
        "data = yf.download(tickers, start='2012-01-01', end='2022-01-01')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLrAm1ybu2vf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cleaning the data\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Normalizing the data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data['Close'].values.reshape(-1, 1))\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "train_size = int(len(scaled_data) * 0.8)\n",
        "test_size = len(scaled_data) - train_size\n",
        "train_data, test_data = scaled_data[0:train_size,:], scaled_data[train_size:len(scaled_data),:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4yLP9Z7u9Si"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Defining the function to create the dataset with look-back\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back, 0])\n",
        "    return np.array(dataX), np.array(dataY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC6Ebnj-vFqB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Creating the dataset with look-back\n",
        "look_back = 60\n",
        "X_train, y_train = create_dataset(train_data, look_back)\n",
        "X_test, y_test = create_dataset(test_data, look_back)\n",
        "\n",
        "# Reshaping the input to be [samples, time steps, features]\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Defining the LSTM model with layered neural network\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj-l6Y2dvJxU",
        "outputId": "4c090cbb-9b27-46be-9cfb-59dc7f59a968"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Fitting the model to the training data\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoUDUn_1v0ou",
        "outputId": "38d37776-ca3a-4f7e-cb0c-21f0f4a01ec8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Predicting the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculating the root mean squared error\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print('Root Mean Squared Error:', rmse)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = tf.keras.metrics.Accuracy()\n",
        "accuracy.update_state(tf.math.round(y_test), tf.math.round(y_pred))\n",
        "acc = accuracy.result().numpy()\n",
        "print('Accuracy:', acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1YjfYVusNE4",
        "outputId": "9e17482e-b3ed-442a-bb41-70d1d25ed019"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pickle\n",
        "  \n",
        "# Save the trained model as a pickle string.\n",
        "saved_model = pickle.dumps(model)\n",
        "\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "import joblib\n",
        "  \n",
        "  \n",
        "# Save the model as a pickle in a file\n",
        "joblib.dump(saved_model, 'model1_lstm_full.pkl')\n",
        "\n",
        "#model_yaml = model.to_yaml()\n",
        "#with open(\"model.yaml\", \"w\") as yaml_file:\n",
        "#    yaml_file.write(model_yaml)\n",
        "# serialize weights to HDF5\n",
        "#model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QziDoUFhsrJk"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4OG2QpIUNK-",
        "outputId": "3b3ef6f0-bab5-4036-fd5b-cadbf75ad276"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Ensemble approach with Random Forest, Gradient Boosting, and AdaBoost\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "gb_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "ada_reg = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fitting the ensemble model to the training data\n",
        "rf_reg.fit(X_train.reshape(-1, X_train.shape[1]), y_train)\n",
        "gb_reg.fit(X_train.reshape(-1, X_train.shape[1]), y_train)\n",
        "ada_reg.fit(X_train.reshape(-1, X_train.shape[1]), y_train)\n",
        "\n",
        "# Predicting the test data using ensemble approach\n",
        "y_pred_rf = rf_reg.predict(X_test.reshape(-1, X_test.shape[1]))\n",
        "y_pred_gb = gb_reg.predict(X_test.reshape(-1, X_test.shape[1]))\n",
        "y_pred_ada = ada_reg.predict(X_test.reshape(-1, X_test.shape[1]))\n",
        "y_pred_ensemble = (y_pred_rf + y_pred_gb + y_pred_ada) / 3\n",
        "\n",
        "# Calculating the root mean squared error using ensemble approach\n",
        "rmse_ensemble = np.sqrt(mean_squared_error(y_test, y_pred_ensemble))\n",
        "print('Ensemble Root Mean Squared Error:', rmse_ensemble)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrCUggNG8x8r"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pickle\n",
        "  \n",
        "# Save the trained model as a pickle string.\n",
        "saved_model = pickle.dumps(model)\n",
        "\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "import joblib\n",
        "  \n",
        "  \n",
        "# Save the model as a pickle in a file\n",
        "joblib.dump(ada_reg, 'model_ensemble_full.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtoDz2ULaTe9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculating the root mean squared error\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred_ada))\n",
        "print('Root Mean Squared Error:', rmse)\n",
        "# Calculating accuracy\n",
        "accuracy = tf.keras.metrics.Accuracy()\n",
        "accuracy.update_state(tf.math.round(y_test), tf.math.round(y_pred_ada))\n",
        "acc = accuracy.result().numpy()\n",
        "print('Accuracy:', acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1cX7pHtqzW7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pickle\n",
        "  \n",
        "# Save the trained model as a pickle string.\n",
        "saved_model = pickle.dumps(model)\n",
        "\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "import joblib\n",
        "  \n",
        "  \n",
        "# Save the model as a pickle in a file\n",
        "joblib.dump(saved_model, 'model2_lstm_full.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J38oEHOkttAM",
        "outputId": "24297989-2c9a-487d-e61f-e0eb7aae64f4"
      },
      "outputs": [],
      "source": [
        "print(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyaFebEB9AGu",
        "outputId": "65e15542-dd89-42d2-e23d-5db64ed851cb"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "\n",
        "# Defining the list of tickers\n",
        "tickers = ['SPY']\n",
        "\n",
        "# Collecting the data\n",
        "data = yf.download(tickers, start='2022-01-01', end='2023-04-12')\n",
        "\n",
        "# Cleaning the data\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Normalizing the data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data['Close'].values.reshape(-1, 1))\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "train_size = int(len(scaled_data) * 0.8)\n",
        "test_size = len(scaled_data) - train_size\n",
        "train_data, test_data = scaled_data[0:train_size,:], scaled_data[train_size:len(scaled_data),:]\n",
        "\n",
        "# Defining the function to create the dataset with look-back\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back, 0])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "# Creating the dataset with look-back\n",
        "look_back = 60\n",
        "X_test, y_test = create_dataset(test_data, look_back)\n",
        "\n",
        "# Reshaping the input to be [samples, time steps, features]\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Load the saved LSTM model\n",
        "#lstm_model = joblib.load('filename.pkl')\n",
        "\n",
        "# Predicting the test data using the LSTM model\n",
        "#y_pred_lstm = lstm_model.predict(X_test)\n",
        "#\n",
        "# Load the saved ensemble model\n",
        "#ensemble_model = joblib.load('model.pkl')\n",
        "ensemble_model = joblib.load('model2_ensemble_full.pkl')\n",
        "\n",
        "\n",
        "# Predicting the test data using the ensemble model\n",
        "y_pred_ensemble = ensemble_model.predict(X_test.reshape(-1, X_test.shape[1]))\n",
        "\n",
        "# Inverse scaling the predicted values to get the actual stock prices\n",
        "#y_pred_lstm = scaler.inverse_transform(y_pred_lstm)\n",
        "y_pred_ensemble = scaler.inverse_transform(y_pred_ensemble.reshape(-1, 1))\n",
        "\n",
        "# Print the predicted next week's stock prices\n",
        "#print('Predicted next week stock prices using LSTM model:\\n', y_pred_lstm[-1])\n",
        "print('Predicted next week stock prices using Ensemble model:\\n', y_pred_ensemble[-1])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
